{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Wikipédia\n",
    "\n",
    "Langue étudiée: français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "\n",
    "# lib à installer\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_rep(rep):\n",
    "    '''\n",
    "    Vérifier l'existence des répertoires, en créer si besoin\n",
    "    '''\n",
    "    try:\n",
    "        os.makedirs(rep) # on essaie de créer le répertoire. si le répertoire existe déjà, cela renvoie une erreur\n",
    "    except:\n",
    "        pass # en cas d'erreur (le répertoire existe), on fait rien\n",
    "\n",
    "def kw2name(kw):\n",
    "    '''\n",
    "    Formatage de variable\n",
    "    '''\n",
    "    return re.sub('\\W', '_', kw.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de pages d'origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(kw):\n",
    "    '''\n",
    "    Récupération des pages wikipédia\n",
    "    \n",
    "    input:\n",
    "    kw (STR): nom de la page à récupérer\n",
    "    \n",
    "    output:\n",
    "    kw (STR)\n",
    "    page (STR): html de la page récupérée\n",
    "    '''\n",
    "    print('Loading Wikipedia page:', kw)\n",
    "    \n",
    "    global URL # l'url de l'api utilisé, défini dans l'environnement\n",
    "    \n",
    "    PARAMS = {\n",
    "        \"action\": \"parse\", # l'api parse le contenu de la page et renvoie le text\n",
    "        \"page\": kw, # on précise la page \n",
    "        \"format\": \"json\", # l'api renvoie le text en format json\n",
    "        \"prop\":\"text\", # on ne prend que le text\n",
    "        \"section\":0 # on ne prend que l'introduction (première section de la page)\n",
    "    }\n",
    "    \n",
    "    S = requests.Session() # on crée une session avec requests\n",
    "    R = S.get(url=URL, params=PARAMS) # on passe la requête à la session\n",
    "    DATA = R.json() # on récupère le résultat au format json\n",
    "    page = DATA[\"parse\"][\"text\"][\"*\"] # on récupère la page html dans le json renvoyé\n",
    "    \n",
    "    filename = kw2name(kw)\n",
    "    with open('../0_data_wiki/pages_aspirees/{}.html'.format(filename), 'w', encoding='utf8', newline='\\n') as htmlfile:\n",
    "        htmlfile.write(page) # on écrit la page html en local\n",
    "        \n",
    "    return kw, page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_filter(span):\n",
    "    '''\n",
    "    Fonction pour filtrer les balises span\n",
    "    On ne veut pas de balise pour la transcription phonétique (span API) ni les messages d'erreur (span error)\n",
    "    \n",
    "    input:\n",
    "    span (bs.tag): balise span\n",
    "    \n",
    "    output:\n",
    "    bool\n",
    "    '''\n",
    "    try:\n",
    "        return ('API' in span['class']) or ('error' in span['class'])\n",
    "    except KeyError:\n",
    "        print(span)\n",
    "        return True\n",
    "\n",
    "def get_text(p):\n",
    "    '''\n",
    "    Fonction pour nettoyer le html et extraire le texte de chaque paragraphe\n",
    "    '''\n",
    "    if p.sup:\n",
    "        p.sup.extract()\n",
    "    if p.span and span_filter(p.span): # enlever les span inutiles\n",
    "        p.span.extract()\n",
    "    if p.style:\n",
    "        p.style.extract()\n",
    "    text = p.text.strip() # enlever les espaces qui traînent\n",
    "    text = re.sub('\\s+', ' ', text) # enlever les suites d'espace produites lors du nettoyage\n",
    "    if len(text) < 50: # enlever wikipédia machin \"modifier - modifier page...\"\n",
    "        return ''\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def parse_wiki_page(kw_page):\n",
    "    '''\n",
    "    Parsing de la page wikipédia\n",
    "    Une balise p enveloppe un paragraphe. On parse toute l'arborescence html, on prend toutes les balises p, \n",
    "    et on nettoie ces balises pour obtenir une liste de texte brut.\n",
    "    \n",
    "    input:\n",
    "    kw_page (TUPLE): (nom de page, html de page)\n",
    "    page (STR): html de page\n",
    "    \n",
    "    output:\n",
    "    text_list_net (LIST): liste de phrases\n",
    "    '''\n",
    "    \n",
    "    kw, page = kw_page\n",
    "    soup = bs(page, 'html5lib') # on parse la page\n",
    "    p_list = soup.find_all('p') # on récupère toutes les p\n",
    "    text_list = list(map(get_text, p_list)) # on nettoie les balises et on obtient les textes\n",
    "    text_list_net = list(filter(lambda x: x, text_list)) # on enlève les textes vides\n",
    "    \n",
    "    # on filtre les phrases trop courtes \n",
    "    sent_list = []\n",
    "    #_ = [sent_list.append(sent) for text in text_list_net for sent in sent_tokenize(text) if len(wordpunct_tokenize(sent)) > 10]\n",
    "    for text in text_list_net:\n",
    "        for sent in sent_tokenize(text):\n",
    "            if len(wordpunct_tokenize(sent)) <= 13:\n",
    "                print('Invalid sentence:', sent)\n",
    "            elif re.match('Erreur de référence', sent):\n",
    "                print('Invalid sentence:', sent)\n",
    "            else:\n",
    "                sent_list.append(sent)\n",
    "    \n",
    "    filename = kw2name(kw)\n",
    "    with open('../0_data_wiki/textes_bruts/{}.txt'.format(filename), 'w', encoding='utf8', newline='\\n') as txtfile:\n",
    "        txtfile.write('\\n\\n'.join(sent_list)) # on écrit ces textes en local\n",
    "    \n",
    "    return kw, sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_url(url):\n",
    "    '''\n",
    "    Nettoyage d'url\n",
    "    '''\n",
    "    if not url.startswith('http'): # des fois l'url aspiré commence par /url?p=, on coupe donc cette partie\n",
    "        url = url[7:]\n",
    "    url = re.split('&sa=', url)[0] # des morceaux inutiles traînent à la fin de l'url (anti-aspiration google), on les coupe avec regex\n",
    "    if \"%3\" in url:\n",
    "        url = re.split('%3', url)[0]\n",
    "    return url\n",
    "\n",
    "def net_url(url):\n",
    "    if 'wikipedia' in url:\n",
    "        return False\n",
    "    elif 'google' in url:\n",
    "        return False\n",
    "    elif 'youtube' in url:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_url_list(quest, nb_url):\n",
    "    '''\n",
    "    Pour chaque phrase, on fait une requête google pour obtenir une liste d'url\n",
    "    \n",
    "    input:\n",
    "    quest (STR): phrase à chercher\n",
    "    nb_url (INT): nombre d'url à trouver\n",
    "    \n",
    "    output:\n",
    "    url_net (LIST): liste d'url\n",
    "    '''\n",
    "    n = 0 \n",
    "    url_list = []\n",
    "    \n",
    "    while n < nb_url:\n",
    "        gap = uniform(1.0,10.0)\n",
    "        time.sleep(gap)\n",
    "        url = \"http://www.google.fr/search?custom?hl=fr&q={}&start={}\".format('\\\"{}\\\"'.format(quest), n)\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "\n",
    "        soup = bs(data, 'html.parser')\n",
    "        div_main = soup.find('div', attrs={'id':'main'})\n",
    "        divs = div_main.find_all('div', attrs={'class':'kCrYT'})\n",
    "            \n",
    "        url_list += [div.a['href'] for div in divs if div.a]\n",
    "        url_short = list(map(sep_url, url_list))\n",
    "        url_net = list(filter(net_url, url_short))\n",
    "        \n",
    "        n += 10\n",
    "        \n",
    "    return url_net\n",
    "\n",
    "def url_to_name(url):\n",
    "    if re.search('.*archives-ouvertes.*document', url):\n",
    "        req_name = url.rsplit('/', maxsplit=2)[1]\n",
    "        req_name += '.pdf'\n",
    "    elif url.endswith('/'):\n",
    "        req_name = url.rsplit('/', maxsplit=2)[1]\n",
    "    else:\n",
    "        req_name = url.rsplit('/', maxsplit=1)[1]\n",
    "    if '.' not in req_name:\n",
    "        req_name += '.html'\n",
    "        \n",
    "    if len(req_name) > 150:\n",
    "        _ = req_name.split('.')\n",
    "        req_name = '.'.join([_[0][:100] + _[1]])\n",
    "        \n",
    "    req_name = re.sub('[\\\\/*?<>|\\\":]', '', req_name)\n",
    "    return req_name\n",
    "\n",
    "def get_urls(texts, nb_url):\n",
    "    '''\n",
    "    Génération de bdd de nom_de_page - url_list\n",
    "    \n",
    "    input:\n",
    "    texts (DICT): {nom_de_page: list(phrase1, phrase2...)}\n",
    "    nb_url (INT): nombre d'url à récupérer\n",
    "    \n",
    "    output:\n",
    "    url_dict (DICT): {nom_de_page: {phrase1:[url1, url2...]...}}\n",
    "    '''\n",
    "    global output_rep\n",
    "    \n",
    "    url_dict = {}\n",
    "    \n",
    "    for kw, text_list in texts.items(): # pour chaque paire nom_de_page - liste_de_paragraphes\n",
    "        gap = uniform(1.0,10.0)\n",
    "        print(\"{}: {} texts\\nStarting in {}s...\".format(kw, len(text_list), round(gap, 3)))\n",
    "        time.sleep(gap)\n",
    "        \n",
    "        reppath = os.path.join(output_rep, kw2name(kw))\n",
    "        verify_rep(reppath) # on crée un répertoire pour chaque nom de page\n",
    "\n",
    "        excelname = '{}_urls.xlsx'.format(kw2name(kw))\n",
    "        excelpath = os.path.join(reppath, excelname)\n",
    "\n",
    "        df_stats = pd.DataFrame(index=['text', 'nb_url', 'nb_invalid','longueur_text'])\n",
    "        \n",
    "        dfs = []\n",
    "        tmp = {}\n",
    "        \n",
    "        for i, text in enumerate(text_list): # pour chaque paragraphe\n",
    "            url_list = get_url_list(text, nb_url) # on extrait la liste d'url \n",
    "\n",
    "            while len(url_list) < 51:\n",
    "                url_list.append('')\n",
    "            df_url = pd.DataFrame(columns=['url', 'copier_coller', 'longueur', 'type', 'domaine', 'citer_source', 'licence'])\n",
    "            df_url = df_url.assign(url=url_list)\n",
    "\n",
    "            tmp[text] = url_list\n",
    "            filename = 'url_{}.txt'.format(i+1)\n",
    "            filepath = os.path.join(reppath, filename)\n",
    "            with open(filepath, 'w', encoding='utf8', newline='\\n') as urlio:\n",
    "                urlio.write('\\n'.join(url_list)) # on écrit la liste en local\n",
    "\n",
    "            # stocker le contenu des url en local\n",
    "            url_rep = os.path.join(reppath, str(i+1))\n",
    "            verify_rep(url_rep) # on crée le répertoire de sortie\n",
    "\n",
    "            # fichier log pour chaque paragraphe\n",
    "            log_path = os.path.join(reppath, 'log_{}.txt'.format(i+1))\n",
    "            log_io = open(log_path, 'w', encoding='utf8', newline='\\n') \n",
    "\n",
    "            invalid = 0\n",
    "\n",
    "            # on lit chaque url\n",
    "            for url in tqdm(url_list, total=len(url_list), desc=\"Processing text {}...\".format(i+1)): \n",
    "                try:\n",
    "                    req = requests.get(url)\n",
    "                except: # adresse non valide\n",
    "                    log_io.write('Invalid url: {}\\n\\n'.format(url))\n",
    "                    url_id = url_list.index(url)\n",
    "                    df_url.iloc[url_id, 1] = 'Invalid'\n",
    "                    invalid += 1\n",
    "                    continue\n",
    "                if req.status_code == 200: # connexion au serveur aboutie\n",
    "                    req_name = url_to_name(url)\n",
    "                    req_path = os.path.join(url_rep, req_name)\n",
    "                    with open(req_path, 'wb') as req_io:\n",
    "                        req_io.write(req.content)\n",
    "                else: # connexion au serveur non aboutie\n",
    "                    log_io.write('Invalid url: {}\\n'.format(url))\n",
    "                    log_io.write('Status code: {}\\n\\n'.format(req.status_code))\n",
    "                    url_id = url_list.index(url)\n",
    "                    df_url.iloc[url_id, 1] = 'Invalid'\n",
    "                    invalid += 1\n",
    "\n",
    "            df_stats['text_{}'.format(i)] = [text, len(url_list), invalid, len(wordpunct_tokenize(text))]\n",
    "            log_io.close()\n",
    "            dfs.append(df_url)\n",
    "\n",
    "        # écriture excel\n",
    "        with pd.ExcelWriter(excelpath) as writer:\n",
    "            for i, df in enumerate(dfs):\n",
    "                df.to_excel(writer, sheet_name='text_{}'.format(i), encoding='utf8')\n",
    "            df_stats.to_excel(writer, sheet_name='stats', encoding='utf8')\n",
    "\n",
    "        # écriture binaire\n",
    "        pkl_path = os.path.join(reppath, '{}.pkl'.format(kw2name(kw)))\n",
    "        with open(pkl_path, 'wb') as pkl_io:\n",
    "            pickle.dump(tmp, pkl_io)\n",
    "            \n",
    "        # mise à jour url_dict\n",
    "        url_dict[kw] = tmp\n",
    "        \n",
    "    return url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia page: Charles de Gaulle\n",
      "Loading Wikipedia page: Pirates des Caraïbes : Jusqu'au bout du monde\n",
      "Loading Wikipedia page: À la recherche du temps perdu\n",
      "Loading Wikipedia page: Alchimie\n",
      "Loading Wikipedia page: Le Jour de Qingming au bord de la rivière\n",
      "\n",
      "Parsing Wikipedia pages...\n",
      "\n",
      "Invalid sentence: Il est fait prisonnier lors de la Première Guerre mondiale.\n",
      "Invalid sentence: Il dirige le pays à la Libération.\n",
      "Invalid sentence: Johnny DeppOrlando BloomKeira KnightleyGeoffrey Rush\n",
      "Invalid sentence: Pour plus de détails, voir Fiche technique et Distribution\n",
      "Invalid sentence: J.-C. et en Inde dès le VIe siècle.\n",
      "\n",
      "\n",
      "Charles de Gaulle: 27 texts\n",
      "Starting in 9.91s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text 1...: 100%|███████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 274.17it/s]\n"
     ]
    }
   ],
   "source": [
    "output_rep = '../0_data_google_exact'\n",
    "\n",
    "verify_rep('../0_data_wiki/pages_aspirees')\n",
    "verify_rep('../0_data_wiki/textes_bruts')\n",
    "verify_rep(output_rep)\n",
    "\n",
    "URL = \"https://fr.wikipedia.org/w/api.php\" # l'api utilisé\n",
    "\n",
    "KWS = [\n",
    "    'Charles de Gaulle',\n",
    "    'Pirates des Caraïbes : Jusqu\\'au bout du monde',\n",
    "    'À la recherche du temps perdu',\n",
    "    'Alchimie',\n",
    "    'Le Jour de Qingming au bord de la rivière'\n",
    "]\n",
    "\n",
    "\n",
    "url_dict = {}\n",
    "\n",
    "pages = dict(map(get_page, KWS)) \n",
    "# on applique la fonction get_page à tous les éléments de KWS, le résultat est renvoyé sous forme de dictionnaire\n",
    "# dictionnaire {nom_de_page:html_de_page}\n",
    "\n",
    "print('\\nParsing Wikipedia pages...\\n')\n",
    "texts = dict(map(parse_wiki_page, pages.items()))\n",
    "# on applique la fonction parse_page à chaque item du dictionnaire pages, et on récupère le résultat sous forme de dictionnaire\n",
    "# texts: {nom_de_page: list(paragraphe1, paragraphe2...)}\n",
    "\n",
    "nb_url = 50\n",
    "print('\\n')\n",
    "url_dict.update(get_urls(texts, nb_url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
